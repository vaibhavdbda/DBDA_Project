{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ce23c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- region_url: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- condition: string (nullable = true)\n",
      " |-- cylinders: string (nullable = true)\n",
      " |-- fuel: string (nullable = true)\n",
      " |-- odometer: string (nullable = true)\n",
      " |-- title_status: string (nullable = true)\n",
      " |-- transmission: string (nullable = true)\n",
      " |-- VIN: string (nullable = true)\n",
      " |-- drive: string (nullable = true)\n",
      " |-- size: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- paint_color: string (nullable = true)\n",
      " |-- image_url: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- county: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- lat: string (nullable = true)\n",
      " |-- long: string (nullable = true)\n",
      " |-- posting_date: string (nullable = true)\n",
      "\n",
      "+----------------------+---------------------+--------------------+----------------------------+---------------------+-------------------------+-------------------------+--------------------+------------------------+----------------------------+----------------------------+---------------------+--------------------+---------------------------+\n",
      "|region_null_percentage|price_null_percentage|year_null_percentage|manufacturer_null_percentage|model_null_percentage|condition_null_percentage|cylinders_null_percentage|fuel_null_percentage|odometer_null_percentage|title_status_null_percentage|transmission_null_percentage|drive_null_percentage|type_null_percentage|paint_color_null_percentage|\n",
      "+----------------------+---------------------+--------------------+----------------------------+---------------------+-------------------------+-------------------------+--------------------+------------------------+----------------------------+----------------------------+---------------------+--------------------+---------------------------+\n",
      "|   0.21339413604606977| 0.013553052227085376|  0.3279307146710461|           5.315719680360564|   2.7326673735513514|       39.067337409878846|        41.59750623839022|  2.3127885006338045|      2.5838495451755117|           3.440880788947087|          2.2025038599624236|     31.5089862051188|  22.618715436394996|         30.537418382722247|\n",
      "+----------------------+---------------------+--------------------+----------------------------+---------------------+-------------------------+-------------------------+--------------------+------------------------+----------------------------+----------------------------+---------------------+--------------------+---------------------------+\n",
      "\n",
      "root\n",
      " |-- region: string (nullable = false)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- manufacturer: string (nullable = false)\n",
      " |-- model: string (nullable = false)\n",
      " |-- condition: string (nullable = false)\n",
      " |-- cylinders: string (nullable = false)\n",
      " |-- fuel: string (nullable = false)\n",
      " |-- odometer: integer (nullable = true)\n",
      " |-- title_status: string (nullable = false)\n",
      " |-- transmission: string (nullable = false)\n",
      " |-- drive: string (nullable = false)\n",
      " |-- type: string (nullable = false)\n",
      " |-- paint_color: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"UsedCarPricePrediction\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Reading csv file in pyspark dataframe:\n",
    "df = spark.read.csv(r\"D:\\vehicles.csv\",header=True, inferSchema=True)\n",
    "\n",
    "df.printSchema()\n",
    "df.count()\n",
    "\n",
    "#Drop the independent columns :\n",
    "columns_to_delete = ['id', 'url', 'region_url','VIN','image_url','description','county','lat','long','posting_date','size','state']\n",
    "df1 = df.drop(*columns_to_delete)\n",
    "\n",
    "# Show column after deleting:\n",
    "df1.columns\n",
    "\n",
    "#drop duplicated records :\n",
    "df2 = df1.distinct()\n",
    "df2.count()\n",
    "\n",
    "#Calculate the percentage of null values for each column :\n",
    "null_counts = df2.select([sum(col(column).isNull().cast('int')).alias(column) for column in df2.columns])\n",
    "total_rows = df2.count()\n",
    "null_percentages = null_counts.select([((col(column) / total_rows) * 100).alias(column + \"_null_percentage\") for column in df2.columns])\n",
    "null_percentages.show() # show null value %\n",
    "\n",
    "#Drop rows with null values in specified columns :\n",
    "df3 = df2.na.drop(subset=['region','price','year','model','odometer','manufacturer','transmission','title_status','fuel'])\n",
    "df3.count()\n",
    "\n",
    "#Handling Missing Values with Categorical Encoding :\n",
    "df4 = df3.fillna('unknown')\n",
    "\n",
    "#handling cloumns :\n",
    "\n",
    "#manufacturer :\n",
    "# Define the list of top 20 manufacturers\n",
    "manufacturer_values = ['nissan','honda','chevrolet','mercedes-benz','ram','dodge','ford','jeep','toyota','bmw','subaru','volkswagen','kia','cadillac','hyundai','lexus','audi','chrysler','acura','buick']\n",
    "\n",
    "# Use when function to update the 'manufacturer' column\n",
    "df5 = df4.withColumn('manufacturer', \n",
    "                   when(df4['manufacturer'].isin(manufacturer_values), df4['manufacturer'])\n",
    "                   .otherwise('others'))\n",
    "\n",
    "# region :\n",
    "# Count the occurrences of each region value\n",
    "manufacturer_counts = df5.groupBy('region').count()\n",
    "\n",
    "# Sort the counts in descending order and select the top 50 region\n",
    "top_manufacturers = manufacturer_counts.orderBy('count', ascending=False).limit(50)\n",
    "\n",
    "# Extract the top 50 region values\n",
    "manufacturer_values = [row['region'] for row in top_manufacturers.collect()]\n",
    "\n",
    "# Use when function to update the 'region' column\n",
    "df6 = df5.withColumn('region', \n",
    "                     when(df5['region'].isin(manufacturer_values), df5['region'])\n",
    "                     .otherwise('others'))\n",
    "\n",
    "#model :\n",
    "# Count the occurrences of each model value\n",
    "manufacturer_counts = df6.groupBy('model').count()\n",
    "\n",
    "# Sort the counts in descending order and select the top 50 model\n",
    "top_manufacturers = manufacturer_counts.orderBy('count', ascending=False).limit(50)\n",
    "\n",
    "# Extract the top 50 model values\n",
    "manufacturer_values = [row['model'] for row in top_manufacturers.collect()]\n",
    "\n",
    "# Use when function to update the 'model' column\n",
    "df7 = df6.withColumn('model',\n",
    "                     when(df6['model'].isin(manufacturer_values), df6['model'])\n",
    "                     .otherwise('others'))\n",
    "\n",
    "\n",
    "# transmission :\n",
    "names_to_match = ['automatic','manual','other','unknown'] \n",
    "df8 = df7.filter((col(\"transmission\").isin(names_to_match)) )\n",
    "\n",
    "#year :\n",
    "#converting year, odometer, price column type to integer type:\n",
    "\n",
    "df9 = df8.withColumn(\"year\", col('year').cast(\"int\"))\n",
    "df10 = df9.withColumn(\"odometer\", col('odometer').cast(\"int\"))\n",
    "df11 = df10.withColumn(\"price\", col('price').cast(\"int\"))\n",
    "df11.printSchema()\n",
    "\n",
    "# handling outliers :\n",
    "#price:\n",
    "# Calculate quartiles\n",
    "\n",
    "price_percentiles = df11.approxQuantile(\"price\", [0.15, 0.75], 0.01)\n",
    "price_percentile15 = price_percentiles[0]\n",
    "price_percentile75 = price_percentiles[1]\n",
    "\n",
    "# Calculate IQR and upper/lower limits\n",
    "\n",
    "price_iqr = price_percentile75 - price_percentile15\n",
    "price_upper_limit = price_percentile75 + 1.5 * price_iqr\n",
    "price_lower_limit = price_percentile15\n",
    "\n",
    "# Filter DataFrame based on limits\n",
    "df12 = df11.filter((col(\"price\") < price_upper_limit) & (col(\"price\") > price_lower_limit))\n",
    "\n",
    "\n",
    "#odometer:\n",
    "# Calculate percentiles\n",
    "odometer_percentiles = df12.approxQuantile(\"odometer\", [0.05, 0.25, 0.75], 0.01)\n",
    "odometer_percentile05 = odometer_percentiles[0]\n",
    "odometer_percentile25 = odometer_percentiles[1]\n",
    "odometer_percentile75 = odometer_percentiles[2]\n",
    "\n",
    "# Calculate IQR and upper/lower limits\n",
    "odometer_iqr = odometer_percentile75 - odometer_percentile25\n",
    "odometer_upper_limit = odometer_percentile75 + 1.5 * odometer_iqr\n",
    "odometer_lower_limit = odometer_percentile05\n",
    "\n",
    "# Filter DataFrame based on limits\n",
    "df13 = df12.filter((col(\"odometer\") < odometer_upper_limit) & (col(\"odometer\") > odometer_lower_limit))\n",
    "\n",
    "#year : removing year before 1996 based on barplot distribution :\n",
    "# Filter DataFrame based on the condition\n",
    "df14 = df13.where(df13['year'] > 1996)\n",
    "\n",
    "# Drop records where year column has a value of 2022\n",
    "df15 = df14.filter(df['year'] != 2022)\n",
    "\n",
    "# adding new column 'car_age' based on purchase year and till 2022\n",
    "df16 = df15.withColumn('car_age', 2024 - col('year'))\n",
    "\n",
    "#droping year :\n",
    "df17 = df16.drop('year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4338842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler,StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression,RandomForestRegressor,DecisionTreeRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e7e6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df17.randomSplit([0.8, 0.2],seed=23)\n",
    "numerical=[\"odometer\",\"car_age\"]\n",
    "numerical_vector_assembler = VectorAssembler(inputCols=numerical,outputCol='numerical_feature_vector')\n",
    "\n",
    "train = numerical_vector_assembler.transform(train)\n",
    "test = numerical_vector_assembler.transform(test)\n",
    "\n",
    "scaler = StandardScaler(inputCol='numerical_feature_vector',outputCol='scaled_numerical_feature_vector',withStd=True, withMean=True)\n",
    "\n",
    "scaler = scaler.fit(train)\n",
    "\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "indexer = StringIndexer(inputCols=['manufacturer','model','condition','cylinders','fuel','title_status','transmission','drive','type','paint_color','region'],\n",
    "                        outputCols=['manufacturer_index','m_i','co_i','cy_i','f_i','ts_i','tr_i','d_i','ty_i','p_i','r_i'],handleInvalid=\"keep\")\n",
    "\n",
    "indexer = indexer.fit(train)\n",
    "train = indexer.transform(train)\n",
    "test = indexer.transform(test)\n",
    "\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(inputCols=['manufacturer_index','m_i','co_i','cy_i','f_i','ts_i','tr_i','d_i','ty_i','p_i','r_i'],\n",
    "                                outputCols=['manufacturer_index_h','m_i_h','co_i_h','cy_i_h','f_i_h','ts_i_h','tr_i_h','d_i_h','ty_i_h','p_i_h','r_i_h'])\n",
    "\n",
    "one_hot_encoder = one_hot_encoder.fit(train)\n",
    "\n",
    "train = one_hot_encoder.transform(train)\n",
    "test = one_hot_encoder.transform(test)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['scaled_numerical_feature_vector',\n",
    "                                       'manufacturer_index_h','m_i_h','co_i_h','cy_i_h','f_i_h','ts_i_h','tr_i_h','d_i_h','ty_i_h','p_i_h','r_i_h'],\n",
    "                            outputCol='final_feature_vector')\n",
    "\n",
    "train = assembler.transform(train)\n",
    "test = assembler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aa3c17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 on test data = 0.743116\n",
      "Mean squared error: 38948822.38252994\n",
      "Mean absolute error: 4640.350445107468\n",
      "Root mean squared error: 6240.899164585976\n",
      "+--------------------+-----+-------------------+\n",
      "|final_feature_vector|price|         prediction|\n",
      "+--------------------+-----+-------------------+\n",
      "|(186,[0,1,3,23,76...| 3900|  6108.907040803881|\n",
      "|(186,[0,1,3,23,74...| 4500| 112.30122763755207|\n",
      "|(186,[0,1,6,23,75...| 4750| -7905.007637644871|\n",
      "|(186,[0,1,16,23,7...| 4900| 1440.0850407869839|\n",
      "|(186,[0,1,2,28,74...| 5991|-171.45731900099054|\n",
      "|(186,[0,1,20,23,7...| 6500|  9519.871136631193|\n",
      "|(186,[0,1,6,30,75...| 6500|  12724.29484689311|\n",
      "|(186,[0,1,2,23,76...| 7200|  15326.06860554286|\n",
      "|(186,[0,1,5,23,76...| 7500| -5820.476367295847|\n",
      "|(186,[0,1,6,23,76...| 8200| 12847.421796108987|\n",
      "|(186,[0,1,10,23,7...| 8499|  9910.185377823325|\n",
      "|(186,[0,1,3,23,76...| 9500| 12604.677624211541|\n",
      "|(186,[0,1,3,23,76...| 9988|  16891.53947227353|\n",
      "|(186,[0,1,6,23,76...|11000| 15118.939766590569|\n",
      "|(186,[0,1,5,39,76...|12900| 20290.830348235606|\n",
      "|(186,[0,1,6,23,76...|13000|  12450.21420214804|\n",
      "|(186,[0,1,3,23,76...|13500|   4310.16421087502|\n",
      "|(186,[0,1,4,44,74...|13600|  11902.57259040937|\n",
      "|(186,[0,1,18,23,7...|13999|  12169.10520529325|\n",
      "|(186,[0,1,8,23,76...|14888| 19679.919805621303|\n",
      "+--------------------+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(featuresCol='final_feature_vector',labelCol='price')\n",
    "lr=lr.fit(train)\n",
    "#pred_test_df = lr.transform(test).withColumnRenamed('prediction', 'predicted_vehicle_value')\n",
    "#result=lr.evaluate(test)\n",
    "\n",
    "#pred_train_df = lr.transform(train).withColumnRenamed('prediction','predicted_vehicle_value')\n",
    "#pred_test_df = lr.transform(test).withColumnRenamed('prediction', 'predicted_vehicle_value')\n",
    "\n",
    "#result=lr.evaluate(test)\n",
    "#print(result.r2)\n",
    "\n",
    "#unlabeled_data=test.select(\"final_feature_vector\")\n",
    "#predictions=lr.transform(unlabeled_data)\n",
    "#predictions.show()\n",
    "\n",
    "\n",
    "# Step 7: Make predictions on the testing data\n",
    "predictions = lr.transform(test)\n",
    "\n",
    "# Step 8: Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "mse = evaluator.evaluate(predictions, {evaluator.metricName: \"mse\"})\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"R2 on test data = %g\" % r2)\n",
    "print(\"Mean squared error:\", mse)\n",
    "print(\"Mean absolute error:\", mae)\n",
    "print(\"Root mean squared error:\", rmse)\n",
    "\n",
    "# Show predictions\n",
    "predictions.select(\"final_feature_vector\", \"price\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef8d85c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 on test data = 0.686468\n",
      "Mean squared error: 47537754.49670209\n",
      "Mean absolute error: 5159.37308984287\n",
      "Root mean squared error: 6894.762831069833\n",
      "+--------------------+-----+------------------+\n",
      "|final_feature_vector|price|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(186,[0,1,3,23,76...| 3900|10114.175231275975|\n",
      "|(186,[0,1,3,23,74...| 4500|10376.878064782726|\n",
      "|(186,[0,1,6,23,75...| 4750| 8146.762035620438|\n",
      "|(186,[0,1,16,23,7...| 4900| 9500.227944210896|\n",
      "|(186,[0,1,2,28,74...| 5991| 8780.978165985589|\n",
      "|(186,[0,1,20,23,7...| 6500| 9155.349491119863|\n",
      "|(186,[0,1,6,30,75...| 6500|  9850.82213390965|\n",
      "|(186,[0,1,2,23,76...| 7200| 18495.53578962089|\n",
      "|(186,[0,1,5,23,76...| 7500|  9921.94636707278|\n",
      "|(186,[0,1,6,23,76...| 8200|19053.153481524063|\n",
      "|(186,[0,1,10,23,7...| 8499|10352.797791822859|\n",
      "|(186,[0,1,3,23,76...| 9500|20262.772349341856|\n",
      "|(186,[0,1,3,23,76...| 9988|11245.759102995926|\n",
      "|(186,[0,1,6,23,76...|11000|14016.323812685529|\n",
      "|(186,[0,1,5,39,76...|12900| 18677.01964908055|\n",
      "|(186,[0,1,6,23,76...|13000| 12496.36465548399|\n",
      "|(186,[0,1,3,23,76...|13500| 9986.787917850237|\n",
      "|(186,[0,1,4,44,74...|13600| 9500.227944210896|\n",
      "|(186,[0,1,18,23,7...|13999|21446.370565001613|\n",
      "|(186,[0,1,8,23,76...|14888| 22266.72130610388|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(featuresCol='final_feature_vector',labelCol='price')\n",
    "rf = rf.fit(train)\n",
    "\n",
    "predictions = rf.transform(test)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "# Evaluate the model\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "mse = evaluator.evaluate(predictions, {evaluator.metricName: \"mse\"})\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"R2 on test data = %g\" % r2)\n",
    "print(\"Mean squared error:\", mse)\n",
    "print(\"Mean absolute error:\", mae)\n",
    "print(\"Root mean squared error:\", rmse)\n",
    "\n",
    "# Show predictions\n",
    "predictions.select(\"final_feature_vector\", \"price\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4bd4677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 on test data = 0.650171\n",
      "Mean squared error: 53041059.0453581\n",
      "Mean absolute error: 5393.497656813513\n",
      "Root mean squared error: 7282.929290152288\n",
      "+--------------------+-----+------------------+\n",
      "|final_feature_vector|price|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(186,[0,1,3,23,76...| 3900|10982.724018589934|\n",
      "|(186,[0,1,3,23,74...| 4500|10982.724018589934|\n",
      "|(186,[0,1,6,23,75...| 4750| 7788.588836394521|\n",
      "|(186,[0,1,16,23,7...| 4900| 7788.588836394521|\n",
      "|(186,[0,1,2,28,74...| 5991|12500.075196344424|\n",
      "|(186,[0,1,20,23,7...| 6500|10982.724018589934|\n",
      "|(186,[0,1,6,30,75...| 6500| 7788.588836394521|\n",
      "|(186,[0,1,2,23,76...| 7200| 20115.38405162984|\n",
      "|(186,[0,1,5,23,76...| 7500| 7788.588836394521|\n",
      "|(186,[0,1,6,23,76...| 8200|15833.057570977919|\n",
      "|(186,[0,1,10,23,7...| 8499| 7788.588836394521|\n",
      "|(186,[0,1,3,23,76...| 9500| 20115.38405162984|\n",
      "|(186,[0,1,3,23,76...| 9988|10982.724018589934|\n",
      "|(186,[0,1,6,23,76...|11000|11405.149932157396|\n",
      "|(186,[0,1,5,39,76...|12900|22734.945972495087|\n",
      "|(186,[0,1,6,23,76...|13000|11405.149932157396|\n",
      "|(186,[0,1,3,23,76...|13500| 7788.588836394521|\n",
      "|(186,[0,1,4,44,74...|13600| 7788.588836394521|\n",
      "|(186,[0,1,18,23,7...|13999| 20115.38405162984|\n",
      "|(186,[0,1,8,23,76...|14888|15833.057570977919|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define Decision Tree Regressor\n",
    "dt = DecisionTreeRegressor(featuresCol='final_feature_vector',\n",
    "                      labelCol='price')\n",
    "\n",
    "# Train the model\n",
    "dt = dt.fit(train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = dt.transform(test)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "mse = evaluator.evaluate(predictions, {evaluator.metricName: \"mse\"})\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"R2 on test data = %g\" % r2)\n",
    "print(\"Mean squared error:\", mse)\n",
    "print(\"Mean absolute error:\", mae)\n",
    "print(\"Root mean squared error:\", rmse)\n",
    "\n",
    "# Show predictions\n",
    "predictions.select(\"final_feature_vector\", \"price\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1270c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 on test data = 0.743116\n",
      "Mean squared error: 38948809.745570175\n",
      "Mean absolute error: 4640.338939907425\n",
      "Root mean squared error: 6240.8981521548785\n",
      "+--------------------+-----+-------------------+\n",
      "|final_feature_vector|price|         prediction|\n",
      "+--------------------+-----+-------------------+\n",
      "|(186,[0,1,3,23,76...| 3900|  6109.265210978125|\n",
      "|(186,[0,1,3,23,74...| 4500| 112.87396984635416|\n",
      "|(186,[0,1,6,23,75...| 4750| -7904.114396578705|\n",
      "|(186,[0,1,16,23,7...| 4900| 1440.7160952953163|\n",
      "|(186,[0,1,2,28,74...| 5991|-171.09320751106134|\n",
      "|(186,[0,1,20,23,7...| 6500|  9519.996705272728|\n",
      "|(186,[0,1,6,30,75...| 6500| 12724.176367756787|\n",
      "|(186,[0,1,2,23,76...| 7200| 15326.391892851847|\n",
      "|(186,[0,1,5,23,76...| 7500| -5819.570609181599|\n",
      "|(186,[0,1,6,23,76...| 8200| 12847.224991158264|\n",
      "|(186,[0,1,10,23,7...| 8499|  9910.534971907164|\n",
      "|(186,[0,1,3,23,76...| 9500|  12604.76843654692|\n",
      "|(186,[0,1,3,23,76...| 9988| 16891.541002381928|\n",
      "|(186,[0,1,6,23,76...|11000| 15118.846256434274|\n",
      "|(186,[0,1,5,39,76...|12900| 20290.583865467703|\n",
      "|(186,[0,1,6,23,76...|13000| 12450.250130030143|\n",
      "|(186,[0,1,3,23,76...|13500|  4310.759846709567|\n",
      "|(186,[0,1,4,44,74...|13600| 11902.704956736081|\n",
      "|(186,[0,1,18,23,7...|13999| 12169.749469461854|\n",
      "|(186,[0,1,8,23,76...|14888| 19679.996038564317|\n",
      "+--------------------+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define Ridge Regression model\n",
    "ridge = LinearRegression(featuresCol=\"final_feature_vector\", labelCol=\"price\", elasticNetParam=0.0, regParam=0.5) # regParam is the regularization parameter for Ridge Regression\n",
    "\n",
    "# Train the model\n",
    "ridge_model = ridge.fit(train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = ridge_model.transform(test)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "mse = evaluator.evaluate(predictions, {evaluator.metricName: \"mse\"})\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "print(\"R2 on test data = %g\" % r2)\n",
    "print(\"Mean squared error:\", mse)\n",
    "print(\"Mean absolute error:\", mae)\n",
    "print(\"Root mean squared error:\", rmse)\n",
    "# Show predictions\n",
    "predictions.select(\"final_feature_vector\", \"price\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ce38325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 on test data = 0.743115\n",
      "Mean squared error: 38949005.4006498\n",
      "Mean absolute error: 4640.342148746028\n",
      "Root mean squared error: 6240.913827369338\n",
      "+--------------------+-----+-------------------+\n",
      "|final_feature_vector|price|         prediction|\n",
      "+--------------------+-----+-------------------+\n",
      "|(186,[0,1,3,23,76...| 3900|  6109.338796938748|\n",
      "|(186,[0,1,3,23,74...| 4500| 111.15723483642068|\n",
      "|(186,[0,1,6,23,75...| 4750| -7903.279073356662|\n",
      "|(186,[0,1,16,23,7...| 4900| 1439.1016819446595|\n",
      "|(186,[0,1,2,28,74...| 5991|-173.57784649423047|\n",
      "|(186,[0,1,20,23,7...| 6500|  9518.195562997218|\n",
      "|(186,[0,1,6,30,75...| 6500| 12722.274636017126|\n",
      "|(186,[0,1,2,23,76...| 7200| 15325.396707734355|\n",
      "|(186,[0,1,5,23,76...| 7500| -5823.042511518564|\n",
      "|(186,[0,1,6,23,76...| 8200| 12848.318277258508|\n",
      "|(186,[0,1,10,23,7...| 8499|  9910.226508231712|\n",
      "|(186,[0,1,3,23,76...| 9500| 12605.221170141886|\n",
      "|(186,[0,1,3,23,76...| 9988|  16891.04591368686|\n",
      "|(186,[0,1,6,23,76...|11000|  15121.45301514355|\n",
      "|(186,[0,1,5,39,76...|12900|  20290.72448599188|\n",
      "|(186,[0,1,6,23,76...|13000|  12450.80508219689|\n",
      "|(186,[0,1,3,23,76...|13500|  4308.596733825454|\n",
      "|(186,[0,1,4,44,74...|13600|  11899.32227025209|\n",
      "|(186,[0,1,18,23,7...|13999| 12169.675130186039|\n",
      "|(186,[0,1,8,23,76...|14888|  19679.96431264402|\n",
      "+--------------------+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define Ridge Regression model\n",
    "lasso = LinearRegression(featuresCol=\"final_feature_vector\", labelCol=\"price\", elasticNetParam=1.0, regParam=0.1) # regParam is the regularization parameter for Ridge Regression\n",
    "\n",
    "# Train the model\n",
    "las_model = lasso.fit(train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = las_model.transform(test)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "mse = evaluator.evaluate(predictions, {evaluator.metricName: \"mse\"})\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "print(\"R2 on test data = %g\" % r2)\n",
    "print(\"Mean squared error:\", mse)\n",
    "print(\"Mean absolute error:\", mae)\n",
    "print(\"Root mean squared error:\", rmse)\n",
    "# Show predictions\n",
    "predictions.select(\"final_feature_vector\", \"price\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a0dd7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 on test data = 0.842289\n",
      "Mean squared error: 23912227.2681711\n",
      "Mean absolute error: 3388.127346053956\n",
      "Root mean squared error: 4890.013013088114\n",
      "+--------------------+-----+------------------+\n",
      "|final_feature_vector|price|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(186,[0,1,3,23,76...| 3900|6808.4406292880785|\n",
      "|(186,[0,1,3,23,74...| 4500|7285.7724225155625|\n",
      "|(186,[0,1,6,23,75...| 4750| 4634.417508310227|\n",
      "|(186,[0,1,16,23,7...| 4900| 7640.617234001238|\n",
      "|(186,[0,1,2,28,74...| 5991| 4349.747365716682|\n",
      "|(186,[0,1,20,23,7...| 6500|  8255.21521167016|\n",
      "|(186,[0,1,6,30,75...| 6500| 7822.015681993523|\n",
      "|(186,[0,1,2,23,76...| 7200|14452.627643046984|\n",
      "|(186,[0,1,5,23,76...| 7500| 7428.094462120806|\n",
      "|(186,[0,1,6,23,76...| 8200|10971.939164453655|\n",
      "|(186,[0,1,10,23,7...| 8499| 8915.405505002782|\n",
      "|(186,[0,1,3,23,76...| 9500| 12071.43422107346|\n",
      "|(186,[0,1,3,23,76...| 9988| 12383.41063188269|\n",
      "|(186,[0,1,6,23,76...|11000| 12906.71767793949|\n",
      "|(186,[0,1,5,39,76...|12900|19281.882383599812|\n",
      "|(186,[0,1,6,23,76...|13000|10660.767625096967|\n",
      "|(186,[0,1,3,23,76...|13500|   7821.8143534644|\n",
      "|(186,[0,1,4,44,74...|13600|10516.122324672337|\n",
      "|(186,[0,1,18,23,7...|13999|11156.768675800913|\n",
      "|(186,[0,1,8,23,76...|14888|16184.149045998853|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train GBoost model\n",
    "gbt = GBTRegressor(featuresCol=\"final_feature_vector\", labelCol=\"price\", maxIter=175)\n",
    "model = gbt.fit(train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "mse = evaluator.evaluate(predictions, {evaluator.metricName: \"mse\"})\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "print(\"R2 on test data = %g\" % r2)\n",
    "print(\"Mean squared error:\", mse)\n",
    "print(\"Mean absolute error:\", mae)\n",
    "print(\"Root mean squared error:\", rmse)\n",
    "# Show predictions\n",
    "predictions.select(\"final_feature_vector\", \"price\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f00179e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mC:\\spark\\spark-3.3.4-bin-hadoop2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\clientserver.py:503\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 503\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39msendall(command\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] An existing connection was forcibly closed by the remote host",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mC:\\spark\\spark-3.3.4-bin-hadoop2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "File \u001b[1;32mC:\\spark\\spark-3.3.4-bin-hadoop2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\clientserver.py:506\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    505\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending or receiving.\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 506\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, proto\u001b[38;5;241m.\u001b[39mERROR_ON_SEND)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mPy4JNetworkError\u001b[0m: Error while sending",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 23\u001b[0m\n\u001b[0;32m     13\u001b[0m data \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     14\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.743116\u001b[39m,\u001b[38;5;241m38948822.382529\u001b[39m,\u001b[38;5;241m4640.35044\u001b[39m,\u001b[38;5;241m6240.89916\u001b[39m),\n\u001b[0;32m     15\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandomForest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m0.686468\u001b[39m,\u001b[38;5;241m47537754.496702\u001b[39m,\u001b[38;5;241m5159.3730898\u001b[39m,\u001b[38;5;241m6894.7628310\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradientBoosting\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m0.842289\u001b[39m,\u001b[38;5;241m23912227.2681711\u001b[39m,\u001b[38;5;241m3388.12734605\u001b[39m,\u001b[38;5;241m4890.013013088\u001b[39m)\n\u001b[0;32m     20\u001b[0m ]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Create a DataFrame using the schema and data\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(data, schema)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Show the DataFrame\u001b[39;00m\n\u001b[0;32m     26\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mC:\\spark\\spark-3.3.4-bin-hadoop2\\python\\pyspark\\sql\\session.py:873\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    871\u001b[0m SparkSession\u001b[38;5;241m.\u001b[39m_activeSession \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSparkSession\u001b[38;5;241m.\u001b[39msetActiveSession(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession)\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, DataFrame):\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata is already a DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\spark\\spark-3.3.4-bin-hadoop2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1709\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[0;32m   1707\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[1;32m-> 1709\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(\n\u001b[0;32m   1710\u001b[0m     proto\u001b[38;5;241m.\u001b[39mREFLECTION_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m   1711\u001b[0m     proto\u001b[38;5;241m.\u001b[39mREFL_GET_UNKNOWN_SUB_COMMAND_NAME \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m   1712\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART)\n\u001b[0;32m   1713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[0;32m   1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[1;32mC:\\spark\\spark-3.3.4-bin-hadoop2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1053\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(retry, connection, pne):\n\u001b[0;32m   1052\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException while sending command.\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 1053\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend_command(command, binary\u001b[38;5;241m=\u001b[39mbinary)\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1055\u001b[0m     logging\u001b[38;5;241m.\u001b[39mexception(\n\u001b[0;32m   1056\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException while sending command.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\spark\\spark-3.3.4-bin-hadoop2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32mC:\\spark\\spark-3.3.4-bin-hadoop2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_new_connection()\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mC:\\spark\\spark-3.3.4-bin-hadoop2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     connection\u001b[38;5;241m.\u001b[39mconnect_to_java_server()\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mC:\\spark\\spark-3.3.4-bin-hadoop2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mconnect((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_port))\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"Model\", StringType(), True),\n",
    "    StructField(\"R2\", DoubleType(), True),\n",
    "    StructField(\"Mse\", DoubleType(), True),\n",
    "    StructField(\"Mae\",DoubleType(),True),\n",
    "    StructField(\"Rmse\",DoubleType(),True)\n",
    "])\n",
    "\n",
    "# Define the data for the DataFrame\n",
    "data = [\n",
    "    (\"Linear\", 0.743116,38948822.382529,4640.35044,6240.89916),\n",
    "    (\"RandomForest\",0.686468,47537754.496702,5159.3730898,6894.7628310),\n",
    "    (\"DecisionTree\", 0.650171,53041059.0453581, 5393.4976568,7282.929290),\n",
    "    (\"Ridge\",0.743116,38948809.745570,4640.3389399,6240.8981521),\n",
    "    (\"Lasso\",0.743115,38949005.4006498,4640.34214874,6240.9138273),\n",
    "    (\"GradientBoosting\",0.842289,23912227.2681711,3388.12734605,4890.013013088)\n",
    "]\n",
    "\n",
    "# Create a DataFrame using the schema and data\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7e1982",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, [5, 10, 15])\n",
    "             .addGrid(gbt.maxBins, [20, 30])\n",
    "             .build())\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"r2\")\n",
    "\n",
    "cv = CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "cvModel = cv.fit(train)\n",
    "\n",
    "predictions = cvModel.transform(test)\n",
    "r2= evaluator.evaluate(predictions)\n",
    "print(\"R2 on test data = %g\" % r2)\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
